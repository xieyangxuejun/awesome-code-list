From: <Saved by Blink>
Snapshot-Content-Location: https://bigflake.com/mediacodec/
Subject: Android MediaCodec stuff
Date: Tue, 5 Mar 2018 03:50:12 -0000
MIME-Version: 1.0
Content-Type: multipart/related;
	type="text/html";
	boundary="----MultipartBoundary--ws8xQKGezKsMZBOsjzRL2Xb3VBfoUFb9su2lyKymxl----"

------MultipartBoundary--ws8xQKGezKsMZBOsjzRL2Xb3VBfoUFb9su2lyKymxl----
Content-Type: text/html
Content-ID: <frame-624-b2332ab7-2e45-42d0-874b-0b8208079034@mhtml.blink>
Content-Transfer-Encoding: quoted-printable
Content-Location: https://bigflake.com/mediacodec/

<html><head><meta http-equiv=3D"Content-Type" content=3D"text/html; charset=
=3DUTF-8">
    <title>Android MediaCodec stuff</title>
    <meta content=3D"en-us" http-equiv=3D"Content-Language">
   =20
    <meta name=3D"description" content=3D"Information about the Android Med=
iaCodec class, with sample code.">
	<meta name=3D"viewport" content=3D"width=3Ddevice-width, initial-scale=3D1=
">

	<style type=3D"text/css">
		body {
		   font-family: Arial, Helvetica, sans-serif;
		   color: black;
		   background-color: white;
	 	}
	</style>
</head>

<body>
<h1>Android MediaCodec stuff<br>
<font size=3D"-4">Last update: 2016-06-08</font></h1>

<ul>
<li><a href=3D"https://bigflake.com/mediacodec/#overview">Overview</a>
</li><li><a href=3D"https://bigflake.com/mediacodec/#samples">Samples</a>
</li><li><a href=3D"https://bigflake.com/mediacodec/#faq">FAQ</a>
</li></ul>

<p>This page is about the Android <code>MediaCodec</code>
class, which can be used to encode and decode audio and video data.
It includes a collection of sample code and answers to frequently-asked
questions.</p>

<p>As of Marshmallow (API 23), the
<a href=3D"http://developer.android.com/reference/android/media/MediaCodec.=
html">
official documentation</a> is quite detailed and very useful.  (This is
a <b>huge</b> step up from what was there when this page was created,
and a significant advancement over the semi-reasonable docs in API 21.)  Th=
e
information on that page should be your primary source of information.
The code here is expected to work with API 18+, for broad compatibility.
If you're specifically targeting Lollipop or Marshmallow, you have options
available that aren't shown here.
</p>

<h2><a name=3D"overview">Overview</a></h2>

The <code>MediaCodec</code> class first became available in
Android 4.1 (API 16).  It was added to allow direct access to the media
codecs on the device.  As such, it provides a rather "raw" interface.
While the <code>MediaCodec</code> class exists in both Java and C++
sources, only the former is public.
<p>
In Android 4.3 (API 18),
<code>MediaCodec</code> was expanded to include
a way to provide input through a <code>Surface</code> (via the
<code>createInputSurface</code> method).  This allows input
to come from camera preview or OpenGL ES rendering.
Android 4.3 was also the first release with <code>MediaCodec</code> tests i=
n
<a href=3D"http://source.android.com/compatibility/cts-intro.html">CTS</a>,
which helps ensure consistent behavior between devices.
</p><p>
Android 4.3 also introduced
<a href=3D"http://developer.android.com/reference/android/media/MediaMuxer.=
html">MediaMuxer</a>,
which allows the output of the AVC codec (a raw H.264 elementary stream) to
be converted to .MP4 format, with or without an associated audio stream.

</p><p>
Android 5.0 (API 21) introduced "asynchronous mode", which allows an app
to provide a
<a href=3D"http://developer.android.com/reference/android/media/MediaCodec.=
Callback.html">
callback method</a> that executes as buffers become available.  The bits
of code linked from this page don't take advantage of this, because they
target API 18+.</p>

<h4>Basic Usage</h4>

All uses of the "synchronous" <code>MediaCodec</code> API follow
a basic pattern:
<ul>
  <li>create and configure the <code>MediaCodec</code> object
  </li><li>loop until done:
    <ul>
      <li>if an input buffer is ready:
      <ul>
        <li>read a chunk of input, copy it into the buffer
      </li></ul>
      </li><li>if an output buffer is ready:
      <ul>
        <li>copy the output from the buffer
      </li></ul>
    </li></ul>
  </li><li>release <code>MediaCodec</code> object
</li></ul>
<p>
A single instance of <code>MediaCodec</code> handles one specific type
of data (e.g. MP3 audio or H.264 video), and may encode or decode.  It
operates on "raw" data, so any file headers (e.g. ID3 tags) must be
stripped off.  It does not talk to any higher-level system components;
it will not play your audio through out the speaker or receive a stream
of video over a network.  It just takes buffers of data in and spits
buffers of data out.  (You can use
<a href=3D"http://developer.android.com/reference/android/media/MediaExtrac=
tor.html">MediaExtractor</a>
to strip the wrappers off in most situations.)
</p><p>
Some codecs are very particular about their buffers.  They may need to
have a particular memory alignment, or have a certain minimum or maximum
size, or it may be important to have a certain number of them available.
To accommodate the wide range of possibilities, buffer allocation is
performed by the codecs themselves, rather than the application.  You
do not hand a buffer with data to <code>MediaCodec</code>.  You ask it
for a buffer, and if one is available, you copy the data in.
</p><p>
This may seem contrary to "zero-copy" principles, but in most cases
there will be fewer copies because the codec won't have to copy or
adjust the data to meet its requirements.  In some cases you may be
able to use the buffer directly, e.g. read data from disk or network
directly into the buffer, so a copy won't be necessary.
</p><p>
Input to <code>MediaCodec</code> must be done in "access units".  When
encoding H.264 video that means one frame, when decoding that means a
single NAL unit.  However, it behaves like a stream in the sense that
you can't submit a single chunk and expect a chunk to appear shortly
from the output.  In practice, the codec may want to have several buffers
queued up before producing any output.
</p><p>
It is strongly recommended that you start with sample code, rather than
trying to figure it out from the documentation.


</p><h2><a name=3D"samples">Samples</a></h2>

<p>
<b><a name=3D"EncodeAndMuxTest"></a><a href=3D"https://bigflake.com/mediaco=
dec/EncodeAndMuxTest.java.txt">EncodeAndMuxTest.java</a></b> (requires 4.3,=
 API 18)
</p><blockquote>
    Generates a movie using OpenGL ES.  Uses
    <a href=3D"http://developer.android.com/reference/android/media/MediaCo=
dec.html">MediaCodec</a>
    to encode the movie in an H.264 elementary stream, and
    <a href=3D"http://developer.android.com/reference/android/media/MediaMu=
xer.html">MediaMuxer</a>
    to convert the stream to a .MP4 file.
    <p>
    This was written as if it were a CTS test, but is not part of CTS.
    It should be straightforward to adapt the code to other environments.
</p></blockquote>

<p>
<b><a name=3D"CameraToMpegTest"></a><a href=3D"https://bigflake.com/mediaco=
dec/CameraToMpegTest.java.txt">CameraToMpegTest.java</a></b> (requires 4.3,=
 API 18)
</p><blockquote>
    Records video from the camera preview and encodes it as an MP4 file.  U=
ses
    <a href=3D"http://developer.android.com/reference/android/media/MediaCo=
dec.html">MediaCodec</a>
    to encode the movie in an H.264 elementary stream, and
    <a href=3D"http://developer.android.com/reference/android/media/MediaMu=
xer.html">MediaMuxer</a>
    to convert the stream to a .MP4 file.
    As an added bonus, demonstrates the use of a GLES fragment shader to
    modify the video as it's being recorded.
    <p>
    This was written as if it were a CTS test, but is not part of CTS.
    It should be straightforward to adapt the code to other environments.
</p></blockquote>

<p>
<b><a name=3D"BreakoutPatch"></a><a href=3D"https://bigflake.com/mediacodec=
/0001-Record-game-into-.mp4.patch">Android Breakout game recorder patch</a>=
</b> (requires 4.3, API 18)
</p><blockquote>
    This is a patch for
    <a href=3D"https://github.com/fadden/android-breakout">Android Breakout=
</a>
    v1.0.2 that adds game recording.  While the game is playing at 60fps
    at full screen resolution, a 30fps 720p recording is made with the AVC
    codec.  The file is saved in the app-private data area, e.g.
    <code>/data/data/com.faddensoft.breakout/files/video.mp4</code>.
    <p>
    This is essentially the same as <code>EncodeAndMuxTest.java</code>,
    but it's part of a full app rather than an isolated CTS test.  One key
    difference is in the EGL setup, which is done in a way that allows
    textures to be shared between the display and video contexts.
    <strong>WARNING: this code has a race condition.  See
    <a href=3D"https://github.com/google/grafika/issues/36">this bug report=
</a>
    for details and suggested fix.</strong>
    </p><p>
    Another approach is to render each game frame to an FBO texture,
    and then render a full-screen quad with that twice (once for the
    display, once for the video).  This could be faster for games that
    are expensive to render.  Examples of both approaches can be found in
    <a href=3D"https://github.com/google/grafika">Grafika's RecordFBOActivi=
ty</a>
    class.
</p></blockquote>

<p>
<b><a name=3D"EncodeDecodeTest"></a><a href=3D"https://android.googlesource=
.com/platform/cts/+/jb-mr2-release/tests/tests/media/src/android/media/cts/=
EncodeDecodeTest.java">EncodeDecodeTest.java</a></b> (requires 4.3, API 18)
</p><blockquote>
    CTS test.  There are three tests that do essentially the same thing,
    but in different ways.  Each test will:
    <ul>
        <li>Generate video frames
        </li><li>Encode frames with AVC codec
        </li><li>Decode generated stream
        </li><li>Test decoded frames to see if they match the original
    </li></ul>
    The generation, encoding, decoding, and checking are near-simultaneous:
    frames are generated and fed to the encoder, and data from the encoder
    is fed to the decoder as soon as it becomes available.
    <p>
    The three tests are:
    </p><ol>
        <li>Buffer-to-buffer.  Buffers are software-generated YUV frames in
        <code>ByteBuffer</code> objects, and decoded to the same.  This is
        the slowest (and least portable) approach, but it allows the
        application to examine and modify the YUV data.
        </li><li>Buffer-to-surface.  Encoding is again done from software-g=
enerated
        YUV data in <code>ByteBuffer</code>s, but this time decoding is don=
e
        to a <code>Surface</code>.  Output is checked with OpenGL ES, using
        <code>glReadPixels()</code>.
        </li><li>Surface-to-surface.  Frames are generated with OpenGL ES o=
nto an
        input <code>Surface</code>, and decoded onto a <code>Surface</code>=
.
        This is the fastest approach, but may involve conversions between
        YUV and RGB.
    </li></ol>
    Each test is run at three different resolutions: 720p (1280x720),
    QCIF (176x144), and QVGA (320x240).
    <p>
    The buffer-to-buffer and buffer-to-surface tests can be built
    with Android 4.1 (API 16).  However, because the CTS tests did not
    exist until Android 4.3, a number of devices shipped with broken
    implementations.
    </p><p>
    NOTE: the <code>setByteBuffer()</code> usage may not be strictly
    correct, as it doesn't set "csd-1".
    </p><p>
    (For an example that uses the Android 5.x asynchronous API, see mstorsj=
o's
    <a href=3D"https://github.com/mstorsjo/android-decodeencodetest">androi=
d-decodeencodetest</a>
    project.)
</p></blockquote>

<p>
<b><a name=3D"DecodeEditEncodeTest"></a><a href=3D"https://android.googleso=
urce.com/platform/cts/+/jb-mr2-release/tests/tests/media/src/android/media/=
cts/DecodeEditEncodeTest.java">DecodeEditEncodeTest.java</a></b> (requires =
4.3, API 18)
</p><blockquote>
    CTS test.  The test does the following:
    <ul>
        <li>Generate a series of video frames, and encode them with AVC.
        The encoded data stream is held in memory.
    </li></ul>
    <ul>
        <li>Decode the generated stream with <code>MediaCodec</code>,
        using an output <code>Surface</code>.
        </li><li>Edit the frame (swap green/blue color channels) with an
        OpenGL ES fragment shader.
        </li><li>Encode the frame with <code>MediaCodec</code>, using an in=
put
        <code>Surface</code>.
    </li></ul>
    <ul>
        <li>Decode the edited video stream, verifying the output.
    </li></ul>
    The middle decode-edit-encode pass performs decoding and encoding
    near-simultaneously, streaming frames directly from the decoder to
    the encoder.  The initial generation and final verification are done
    as separate passes on video data held in memory.
    <p>
    Each test is run at three different resolutions: 720p (1280x720),
    QCIF (176x144), and QVGA (320x240).
    </p><p>
    No software-interpreted YUV buffers are used.  Everything goes through
    <code>Surface</code>.  There will be conversions between RGB and YUV
    at certain points; how many and where they happen depends on how the
    drivers are implemented.
    </p><p>
    NOTE: for this and the other CTS tests, you can see related classes
    by editing the class URL.  For example, to see InputSurface and
    OutputSurface, remove "DecodeEditEncodeTest.java" from the URL,
    yielding <a href=3D"https://android.googlesource.com/platform/cts/+/jb-=
mr2-release/tests/tests/media/src/android/media/cts/">
    this directory link</a>.
</p></blockquote>

<p>
<b><a name=3D"ExtractMpegFramesTest"></a><a href=3D"https://bigflake.com/me=
diacodec/ExtractMpegFramesTest.java.txt">ExtractMpegFramesTest.java</a></b>=
 (requires 4.1, API 16)
<br>
<b><a href=3D"https://bigflake.com/mediacodec/ExtractMpegFramesTest_egl14.j=
ava.txt">ExtractMpegFramesTest.java</a></b> (requires 4.2, API 17)
</p><blockquote>
    Extracts the first 10 frames of video from a .mp4 file and saves
    them to individual PNG files in <code>/sdcard/</code>.  Uses
    <a href=3D"http://developer.android.com/reference/android/media/MediaEx=
tractor.html">MediaExtractor</a>
    to extract the CSD data and feed individual access units into a
    <a href=3D"http://developer.android.com/reference/android/media/MediaCo=
dec.html">MediaCodec</a>
    decoder.  The frames are decoded to a <code>Surface</code> created from
    <a href=3D"http://developer.android.com/reference/android/graphics/Surf=
aceTexture.html">SurfaceTexture</a>,
    rendered (off-screen) into a pbuffer, extracted with
    <code>glReadPixels()</code>, and saved to a PNG file with
    <code>Bitmap#compress()</code>.
    <p>Decoding the frame and copying it into a <code>ByteBuffer</code>
    with <code>glReadPixels()</code> takes about 8ms on the Nexus 5,
    easily fast enough to keep pace with 30fps input, but the additional
    steps required to save it to disk as a PNG are expensive (about
    half a second).
    The cost of saving a frame breaks down roughly like this (which
    you can get by modifying the test to extract full-size frames from
    720p video on a Nexus 5, observing the total time required to save
    10 frames, and doing successive runs with later stages removed; or
    by instrumenting with
    <a href=3D"http://developer.android.com/reference/android/os/Trace.html=
">android.os.Trace</a>
    and using <a href=3D"https://bigflake.com/systrace/">systrace</a>):
    </p><ul>
    <li>0.5% video decode (hardware AVC codec)
    </li><li>1.5% <code>glReadPixels()</code> into a direct <code>ByteBuffe=
r</code>
    </li><li>1.5% copying pixel data from <code>ByteBuffer</code> to <code>=
Bitmap</code>
    </li><li>96.5% PNG compression and file I/O
    </li></ul>
    <p>In theory, a <code>Surface</code> from the API 19
    <a href=3D"http://developer.android.com/reference/android/media/ImageRe=
ader.html">ImageReader</a>
    class could be passed to the <code>MediaCodec</code> decoder,
    allowing direct access to the YUV data.  As of Android 4.4, the
    <code>MediaCodec</code> decoder formats are not supported by
    <code>ImageReader</code>.
    </p><p>One possible way to speed up transfer of RGB data would be to
    copy the data asynchronously through a
    <a href=3D"http://www.opengl.org/wiki/Pixel_Buffer_Object">PBO</a>,
    but in the current implementation the transfer time is dwarfed by
    the subsequent PNG activity, so there's little value in doing so.
    </p><p>
    The two versions of the source code function identically.  One was
    written against EGL 1.0, the other EGL 1.4.  EGL 1.4 is a little
    easier to work with and has some features that other examples use,
    but if you want your app to work on Android 4.1 you can't use it.
    </p><p>
    This was written as if it were a CTS test, but is not part of CTS.
    It should be straightforward to adapt the code to other environments.
    (<b>NOTE:</b> if you're having trouble with timeouts in
    <code>awaitNewImage()</code>, see
    <a href=3D"http://stackoverflow.com/questions/22457623/">this article</=
a>.)
</p></blockquote>

<p>
<b><a name=3D"DecoderTest"></a><a href=3D"https://android.googlesource.com/=
platform/cts/+/jb-mr2-release/tests/tests/media/src/android/media/cts/Decod=
erTest.java">DecoderTest.java</a></b> (requires 4.1, API 16)
</p><blockquote>
    CTS test.  Tests decoding pre-recorded audio streams.
</blockquote>

<p>
<b><a name=3D"EncoderTest"></a><a href=3D"https://android.googlesource.com/=
platform/cts/+/jb-mr2-release/tests/tests/media/src/android/media/cts/Encod=
erTest.java">EncoderTest.java</a></b> (requires 4.1, API 16)
</p><blockquote>
    CTS test.  Tests encoding of audio streams.
</blockquote>

<p>
<b><a name=3D"MediaMuxerTest"></a><a href=3D"https://android.googlesource.c=
om/platform/cts/+/kitkat-release/tests/tests/media/src/android/media/cts/Me=
diaMuxerTest.java">MediaMuxerTest.java</a></b> (requires 4.3, API 18)
</p><blockquote>
    CTS test.  Uses MediaMuxer to clone the audio track, video track,
    and audio+video tracks from input to output.
</blockquote>

<p>
<b><a name=3D"screenrecord"></a><a href=3D"http://www.bigflake.com/screenre=
cord/">screenrecord</a></b> (uses non-public native APIs)
</p><blockquote>
    You can access the source code for <code>screenrecord</code>, a develop=
er
    shell command introduced in Android 4.4 (API 19).  It demonstrates the
    use of the native equivalents of <code>MediaCodec</code> and
    <code>MediaMuxer</code> in a pure-native command.  v1.1 uses GLES
    and the native equivalent of <code>SurfaceTexture</code>.
    <p>
    This is FOR REFERENCE ONLY.  Non-public APIs are very likely to break
    between releases and are not guaranteed to have consistent behavior
    across different devices.
</p></blockquote>

<p>
<b><a name=3D"Grafika"></a><a href=3D"https://github.com/google/grafika">Gr=
afika</a></b> (requires 4.3, API 18)
</p><blockquote>
    Test application exercising various graphics &amp; media features.
    Examples include recording and displaying camera preview, recording
    OpenGL ES rendering, decoding multiple videos simultaneously, and
    the use of <code>SurfaceView</code>, <code>GLSurfaceView</code>,
    and <code>TextureView</code>.  Highly unstable.  Tons of fun.
    Unlike most of the samples here, Grafika is a complete application,
    so it's easier to try things yourself.  The EGL/GLES code is also
    more refined, and better suited for inclusion in other projects.
</blockquote>


<h2><a name=3D"faq">FAQ</a></h2>

<p>
<b><a name=3D"q1">Q1.</a></b> How do I play the video streams created by <c=
ode>MediaCodec</code>
with the "video/avc" codec?
</p><p>
<b>A1.</b> The stream created is a raw H.264 elementary stream.  The Totem =
Movie Player
for Linux may work, but many other players won't touch them.  You need to u=
se the
<code>MediaMuxer</code> class to create an MP4 file instead.  See the
<a href=3D"https://bigflake.com/mediacodec/#EncodeAndMuxTest">EncodeAndMuxT=
est sample</a>.

</p><p>&nbsp;</p>
<b><a name=3D"q2">Q2.</a></b> Why does my call to <code>MediaCodec.configur=
e()</code>
fail with an <code>IllegalStateException</code> when I try to create an enc=
oder?
<p>
<b>A2.</b> This is usually because you haven't specified all of the
<a href=3D"http://developer.android.com/reference/android/media/MediaFormat=
.html">mandatory keys</a>
required by the encoder.  See
<a href=3D"http://stackoverflow.com/questions/17233835/illegalstateexceptio=
n-when-mediacodec-configure-android/17243175#17243175">this stackoverflow i=
tem</a>
for an example.

</p><p>&nbsp;</p>
<b><a name=3D"q3">Q3.</a></b> My video decoder is configured but won't acce=
pt
data.  What's wrong?
<p>
<b>A3.</b> A common mistake is neglecting to set the Codec-Specific Data,
mentioned briefly in the documentation, through the keys "csd-0" and
"csd-1".  This is a bunch of raw data with things like Sequence Parameter
Set and Picture Parameter Set; all you usually need to know is that the
<code>MediaCodec</code> encoder generates them and the <code>MediaCodec</co=
de>
decoder wants them.
</p><p>
If you are feeding the output of the encoder to the decoder, you will note
that the first packet you get from the encoder has the
<a href=3D"http://developer.android.com/reference/android/media/MediaCodec.=
html#BUFFER_FLAG_CODEC_CONFIG">BUFFER_FLAG_CODEC_CONFIG</a>
flag set.  You need to make sure you propagate this flag to the decoder,
so that the first buffer the decoder receives does the setup.
Alternatively, you can set the CSD data in the <code>MediaFormat</code>,
and pass this into the decoder via <code>configure()</code>.  You can
see examples of both approaches in the
<a href=3D"https://bigflake.com/mediacodec/#EncodeDecodeTest">EncodeDecodeT=
est sample</a>.
</p><p>
If you're not sure how to set this up, you should probably be using
<a href=3D"http://developer.android.com/reference/android/media/MediaExtrac=
tor.html">MediaExtractor</a>,
which will handle it all for you.

</p><p>&nbsp;</p>
<b><a name=3D"q4">Q4.</a></b> Can I stream data into the decoder?
<p>
<b>A4.</b> Yes and no.  The decoder takes a stream of "access units", which
may not be a stream of bytes.  For the video decoder, this means you need
to preserve the "packet boundaries" established by the encoder (e.g. NAL
units for H.264 video).  For example,
see how the <code>VideoChunks</code> class in the
<a href=3D"https://bigflake.com/mediacodec/#DecodeEditEncodeTest">DecodeEdi=
tEncodeTest sample</a> operates.
You can't just read arbitrary chunks of the file and pass them in.

</p><p>&nbsp;</p>
<b><a name=3D"q5">Q5.</a></b> I'm encoding the output of the camera through
a YUV preview buffer.  Why do the colors look wrong?
<p>
<b>A5.</b> The color formats for the camera output and the
<code>MediaCodec</code> encoder input are different.  Camera supports YV12
(planar YUV 4:2:0) and NV21 (semi-planar YUV 4:2:0).  The
<code>MediaCodec</code> encoders support one or more of:
</p><ul>
    <li>#19 COLOR_FormatYUV420Planar (I420)
    </li><li>#20 COLOR_FormatYUV420PackedPlanar (also I420)
    </li><li>#21 COLOR_FormatYUV420SemiPlanar (NV12)
    </li><li>#39 COLOR_FormatYUV420PackedSemiPlanar (also NV12)
    </li><li>#0x7f000100 COLOR_TI_FormatYUV420PackedSemiPlanar (also also N=
V12)
</li></ul>
I420 has the same general data layout as YV12, but the Cr and Cb planes
are reversed.  Same with NV12 vs. NV21.  So if you try to hand YV12
buffers from the camera to an encoder expecting something else,
you'll see some odd color effects, like in
<a href=3D"http://stackoverflow.com/questions/13703596/mediacodec-and-camer=
a-colorspaces-dont-match">these images</a>.
<p>
As of Android 4.4 (API 19), there is still no common input format.
Nvidia Tegra 3 devices like the Nexus 7 (2012), and Samsung Exynos devices
like the Nexus 10, want <code>COLOR_FormatYUV420Planar</code>.
Qualcomm Adreno devices like the Nexus 4, Nexus 5, and Nexus 7 (2013) want
<code>COLOR_FormatYUV420SemiPlanar</code>.  TI OMAP devices like the Galaxy
Nexus want <code>COLOR_TI_FormatYUV420PackedSemiPlanar</code>.  (This is
based on the format that is returned first when the AVC codec is queried.)
</p><p>

</p><p>
A more portable, and more efficient, approach is to use the API 18
<code>Surface</code> input API, demonstrated in the
<a href=3D"https://bigflake.com/mediacodec/#CameraToMpegTest">CameraToMpegT=
est sample</a>.  The down
side of this is that you have to operate in RGB rather than YUV, which
is a problem for image processing software.  If you can implement the
image manipulation in a fragment shader, perhaps by converting between
RGB and YUV before and after your computations, you can take advantage
of code execution on the GPU.
</p><p>
Note that the <code>MediaCodec</code> <b>decoders</b> may produce data in
ByteBuffers using one of the above formats or in a proprietary format.
For example, devices based on Qualcomm SoCs commonly use
<code>OMX_QCOM_COLOR_FormatYUV420PackedSemiPlanar32m</code> (#2141391876
/ 0x7FA30C04).
</p><p>
Surface input uses <code>COLOR_FormatSurface</code>,
also known as <code>OMX_COLOR_FormatAndroidOpaque</code>
(#2130708361 / 0x7F000789).
For the full list, see <code>OMX_COLOR_FORMATTYPE</code> in
<a href=3D"https://android.googlesource.com/platform/frameworks/native/+/ki=
tkat-release/include/media/openmax/OMX_IVCommon.h">OMX_IVCommon.h</a>.
</p><p>
As of API 21 you can work with an
<a href=3D"http://developer.android.com/reference/android/media/Image.html"=
>Image</a>
object instead.  See the MediaCodec <code>getInputImage()</code> and
<code>getOutputImage()</code> calls.</p>

<p>&nbsp;</p>
<b><a name=3D"q6">Q6.</a></b> What's this <code>EGL_RECORDABLE_ANDROID</cod=
e> flag?
<p>
<b>A6.</b> That tells EGL that the surface it creates must be compatible
with the video codecs.  Without this flag, EGL might use a buffer format
that <code>MediaCodec</code> can't understand.

</p><p>&nbsp;</p>
<b><a name=3D"q7">Q7.</a></b> Can I use the
<a href=3D"http://developer.android.com/reference/android/media/ImageReader=
.html">ImageReader</a>
class with <code>MediaCodec</code>?
<p>
<b>A7.</b> Maybe.  The <code>ImageReader</code> class, added in
Android 4.4 (API 19), provides a handy way to access data in a YUV
surface.  Unfortunately, as of API 19 it only works with buffers from
<code>Camera</code> (though that may have been corrected in API 21, when
MediaCodec added <code>getOutputImage()</code>).  Also,
there was no corresponding <code>ImageWriter</code> class for creating
content until API 23.

</p><p>&nbsp;</p>
<b><a name=3D"q8">Q8.</a></b> Do I have to set a presentation time
stamp when encoding video?
<p>
<b>A8.</b> Yes.  It appears that some devices will drop frames or encode
them at low quality if the presentation time stamp isn't set to a reasonabl=
e
value (see
<a href=3D"http://stackoverflow.com/questions/20475332/">this stackoverflow=
 item</a>).
</p><p>
Remember that the time required by <code>MediaCodec</code> is in
microseconds.  Most timestamps passed around in Java code are in
milliseconds or nanoseconds.

</p><p>&nbsp;</p>
<b><a name=3D"q9">Q9.</a></b> Most of the examples require API 18.  I'm
coding for API 16.  Is there something I should know?
<p>
<b>A9.</b> Yes.  Some key features aren't available until API 18, and
some basic features are more difficult to use in API 16.
</p><p>
If you're decoding video, things don't change much.  As you can see
from the two implementations of
<a href=3D"https://bigflake.com/mediacodec/#ExtractMpegFramesTest">ExtractM=
pegFramesTest</a>,
the newer version of EGL isn't available, but for many applications
that won't matter.
</p><p>
If you're encoding video, things are much worse.  Three key points:
</p><ol>
<li>The <code>MediaCodec</code> encoders don't accept input from a
Surface, so you have to provide the data as raw YUV frames.
</li><li>The layout of the YUV frames varies from device to device, and in
some cases you have to check for specific vendors by name to handle
certain qirks.
</li><li>Some devices may not advertise support for any usable YUV formats
(i.e. they're internal-use only).
</li><li>The <code>MediaMuxer</code> class doesn't exist, so there's no
way to convert the H.264 stream to something that <code>MediaPlayer</code>
(or many desktop players) will accept.  You have to use a 3rd-party
library (perhaps
<a href=3D"http://stackoverflow.com/questions/22996378/">mp4parser</a>).
</li><li>When the <code>MediaMuxer</code> class was introduced in API 18,
the behavior of <code>MediaCodec</code> encoders was changed to emit
<code>INFO_OUTPUT_FORMAT_CHANGED</code> at the start, so that you have
a convenient <code>MediaFormat</code> to feed to the muxer.  On older
versions of Android, this does not happen.
</li></ol>
<p>
This <a href=3D"http://stackoverflow.com/questions/21262797/converting-imag=
es-to-video">stackoverflow item</a>
has additional links and commentary.
</p><p>
The CTS tests for <code>MediaCodec</code> were introduced with API 18
(Android 4.3), which in practice means that's the first release where
the basic features are likely to work consistently across devices.
In particular, pre-4.3 devices have been known to drop the last frame
or scramble PTS values when decoding.

</p><p>&nbsp;</p>
<b><a name=3D"q10">Q10.</a></b> Can I use <code>MediaCodec</code> in the
AOSP emulator?
<p>
<b>A10.</b> Maybe.  The emulator provides a software AVC codec that lacks
certain features, notably input from a Surface (although it appears that th=
is
<a href=3D"https://android.googlesource.com/platform/frameworks/av/+/2edda0=
9a%5E%21/">may now be fixed</a>
in Android 5.0 "Lollipop").
Developing on a physical device will likely be less frustrating.


</p><p>&nbsp;</p>
<b><a name=3D"q11">Q11.</a></b> Why is the output messed up (all zeroes,
too short, etc)?
<p>
<b>A11.</b> The most common mistake is failing to adjust the
<code>ByteBuffer</code> position and limit values.  As of API 19,
<code>MediaCodec</code> does not do this for you.
</p><p>
You need to do something like:
</p><pre>  int bufIndex =3D codec.dequeueOutputBuffer(info, TIMEOUT);
  ByteBuffer outputData =3D outputBuffers[bufIndex];
  if (info.size !=3D 0) {
      outputData.position(info.offset);
      outputData.limit(info.offset + info.size);
  }</pre>
<p>
On the input side, you want to call <code>clear()</code> on the buffer
before copying data into it.


</p><p>&nbsp;</p>
<b><a name=3D"q12">Q12.</a></b> Why am I seeing
<code>storeMetaDataInBuffers</code> failures in the log?
<p>
<b>A12.</b> They look like this (example from a Nexus 5):

</p><pre>E OMXNodeInstance: OMX_SetParameter() failed for StoreMetaDataInBu=
ffers: 0x8000101a
E ACodec  : [OMX.qcom.video.encoder.avc] storeMetaDataInBuffers (output) fa=
iled w/ err -2147483648</pre>

<p>
You can ignore them, they're harmless.


</p><p>&nbsp;</p>

<h2>Further Assistance</h2>

<p>
Please post all questions to <a href=3D"http://stackoverflow.com/">stackove=
rflow</a>
with the <code>android</code> tag (and, for <code>MediaCodec</code>
issues, the <code>mediacodec</code> tag as well).  Comments or feature
requests for the framework or CTS tests should be made on
<a href=3D"http://b.android.com/">the AOSP bug tracker</a>.

<!-- source: http://efforg.github.io/dayofaction-banner/ -->
<!--[if !(lt IE 8)]><!-->

<!--<![endif]-->

<!-- This site is maintained by Andy McFadden (fadden 'at' fadden.com). -->


</p></body><template shadowmode=3D"v0"><shadow></shadow><style></style></te=
mplate></html>
------MultipartBoundary--ws8xQKGezKsMZBOsjzRL2Xb3VBfoUFb9su2lyKymxl------
